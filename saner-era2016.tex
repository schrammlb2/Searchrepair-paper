%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%2345678901234567890123456789012345678901234567890123456789012345678901234567890
%        1         2         3         4         5         6         7         8

\documentclass[conference]{IEEEtran}
\usepackage{authblk}
\usepackage{courier}
\usepackage[utf8]{inputenc}
\usepackage{dirtytalk}
\usepackage{listings}
\usepackage{color}
\usepackage{float}
\usepackage{graphicx}

\restylefloat{table}
\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=tb,
  language=C,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3
}
%\IEEEoverridecommandlockouts                              % This command is only
                                                          % needed if you want to
                                                          % use the \thanks 


% The following packages can be found on http:\\www.ctan.org
%\usepackage{graphics} % for pdf, bitmapped graphics files
%\usepackage{epsfig} % for postscript graphics files
%\usepackage{mathptmx} % assumes new font selection scheme installed
%\usepackage{times} % assumes new font selection scheme installed
%\usepackage{amsmath} % assumes amsmath package installed
%\usepackage{amssymb}  % assumes amsmath package installed

\title{\LARGE \bf
Improving performance of automatic program repair using learned heuristics
}


\author{Liam Schramm}
\affil{Bard College}
\author{Claire Le Goues}
\affil{Carnegie Melon University}


\begin{document}



\maketitle
\thispagestyle{empty}
\pagestyle{empty}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}

Automatic program repair offers the promise of significant reduction in debugging time, but still faces challenges in making the process efficient, accurate, and generalizable enough for application. 
Early generate-and-test systems, such as the genetic programming method GenProg, have had problems creating high-quality patches due to overfitting to the test suite. 
Recent efforts such as Prophet demonstrate that machine learning can be used to develop heuristics about which patches are likely to be correct, dramatically improving speed of search. SearchRepair takes a different approach to accuracy, using larger blocks of human-written code as patches to better constrain repairs. 
However, large blocks of relevant code become scarcer as size increases, leading to large search spaces and high computational costs. 
This project combines Prophet's learning techniques with SearchRepair's larger block size to create a method that is both fast and accurate, leading to higher-quality repairs. 
We propose a novel, fast classifier to quickly identify good candidate patches in the SearchRepair context. 
We use a random forest to quickly classify whether two pieces of code are similar, allowing the search to focus only on the best patch candidates. We report 96\% accuracy on this classification task, which is enough to greatly improve search speed.


\textit{Keywords}-- Automatic program repair, Machine learning, Semantic search.


\end{abstract}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{INTRODUCTION}

Software debugging is a tedious, expensive, and manual process. 
The majority of the cost of most software projects is spent on software maintenance, and the majority of the cost of software maintenance is debugging \cite{Genprog}. 
Automated program repair offers the promise of dramatically lowering or even eliminating these costs. 
However, the fledgling field still struggles with making tools that are practical enough to reach the level of popular usage. 
For an automated repair tool to be not only successful but widely useful, it must be meet certain criteria of efficiency, accuracy, and generality. For our purposes, we define these as:
\begin{itemize}
\item Efficiency: 
	The ability to create and validate a patch for a given bug in a reasonable amount of time. 
	An automated program repair tool must be fast enough that it is cheaper and more convenient to fix the bug automatically than it is to fix it by hand.
	
\item Accuracy: 
The probability that the patch proposed for a given bug is correct. 
Although we would like to be confident enough in the algorithms recommendations that they would not need human oversight, this goal seems far off. 
At present, a more reasonable goal is that a correct patch is high enough  on the list of recommended patches that a human can easily pick it out. 
\item Generality: 
The range of bugs a repair algorithm is able to address. 
While tools that address specific types of bugs may be adept in their particular field, there are simply too many different types of bugs for addressing them with individual programs to be practical. 
Methods that use certain preset types of modifications report that no one modification ever accounts for a large fraction of total repairs \cite{PAR}, \cite{hdrepair}, \cite{SPR}. 
For automated program repair to become highly useful in a real-world environment, it must be general enough to fix a wide range of bugs.
\end{itemize}


Several methods exist for automatic program repair, but all of them encounter problems with at least one of the above criteria. 
Semantic algorithms like DirectFix or Angelix are limited in the bugs they are able to address and sometimes scale poorly \cite{Angelix}. 
Search-based algorithms such as GenProg and SPR offer the possibility of a wider range of patches, but also are very likely to create patches which pass the test suite, but are ultimately incorrect \cite{Angelix}, \cite{SPR} \cite{Genprog}. One reason that search-based methods have this problem is that the search space is effectively infinite, and within this space, there are many more patches that pass all tests (plausible patches) and are incorrect than there are correct patches \cite{searchspace}. 
One solution to this problem is to use a fast, learned heuristic to focus the search on patches that look like human-written patches, which are more likely to be correct \cite{Prophet}. 

Another approach is to encode test cases as semantic constraints and then paste in human-written code blocks as patches if they fit the constraints. The idea is that human-written code blocks are more likely to fit the correct functionality than computer-generated patches because they represent actual human coding patterns. In this paper, we combine this fast heuristic search with the more accurate human code block insertion technique to create a method that is both fast and reliable. 



\section{Background}

Automated program repair is the process of patching bugs in a program given a test suite for that program. 
Currently, there are two common approaches to this problem: generate-and-validate and semantic analysis. 
Generate-and-validate solutions create a large number of candidate patches, and then evaluate them one-at-a-time until an optimal candidate has been chosen. 
This candidate is then tested against the test suite and, if it passes, is accepted. 
If it is not accepted, a new patch is chosen, and the process is repeated until an acceptable patch is chosen, the search space is exhausted, or the time limit is reached. 
Semantic analysis works by extracting a repair constraint through symbolic execution. These constraints are then fed to theorem provers that infer a correct patch, which is then inserted. 

In 2009, GenProg became the first algorithm to successfully perform automated program repair on large-scale real-world problems. 
It uses genetic programming to encode possible patches as variations to the source code. 
Its algorithm works approximately as follows:
\begin{enumerate}
\item Randomly generate a population of potential patches.
\item Run a subset of the test suite on each member of the population. The number of tests each member passes serves as the fitness function.
\item Breed the members of the population with the highest fitness.
\end{enumerate}

In the months after GenProg's publishing, however, it became apparent that the method was not as effective as it had first seemed. 
Several studies found that GenProg frequently \say{fixed} bugs by deleting functionality. 
This is because its fitness function is defined solely by the test suite. 
Any functionality not covered by the test suite is likely to be deleted if any part of it contributes to buggy behavior \cite{plausibility}. 
Since the GenProg stops searching once it has found a patch that passes all the tests in the test suite, this pattern of functionality-deletion often contributes to GenProg failing to find patches that are within its search space \cite{searchspace}.

This problem can also be viewed as an issue of overfitting. 
In a sense, GenProg overfits to the test suite by using it as the sole determinant of whether a patch is correct. 
Thus, it creates patches which perform well on the test suite but poorly fit the actual function of the code. 
Most of the work done in the field since this discovery can be seen as an attempt to avoid overfitting.

Within the generate-and-validate framework, two methods have emerged for improving the quality of patches. 
The first approach is taken by Prophet and SPR. Although both still create incorrect patches about half the time, they reduce the likelihood of this happening by trying to produce a small number of patches that obey a collection of hand-coded rules \cite{SPR}, \cite{Prophet}. 
Each hand-coded rule, or schema, covers a specific type of repair. SPR and Prophet first use a target value search to check whether each schema is capable of generating a patch for the given bug. 
If it is not, the schema is discarded. 
They then generate a search space of patches from the remaining schemas, and test each of the patches in this space.

Since correct patches are sparse in the space of plausible patches, randomly generating solutions is more likely to produce plausible, incorrect patches than correct patches \cite{searchspace}.  
The authors of SPR and Prophet believe that their careful construction of patches is less likely to produce incorrect patches because it utilizes more information, both about the program and about the nature of developer patches.  
Prophet also uses machine learning to select for patches with similar qualities to human patches \cite{Prophet}. 
This feature has several advantages. 
Firstly, it allows Prophet to rank patches without using the test suite, reducing the threat of overfitting. 
Since applying the learner is also much faster than running the test suite, it also serves as a useful search heuristic. 
This allows Prophet to focus its search significantly by looking for patches that share features with the correct patches. 

The other approach, which is taken by SearchRepair, uses an algorithm that combines the search and the semantic analysis paradigms \cite{SearchRepair}. 
\begin{enumerate}
\item Encode a database of human-written code fragments as satisfiability modulo theories (SMT).
\item Locate the bug.
\item Build a functional description of each fragment that describes its input-output profile.
\item Search the database for a code fragment that matches the desired input-output profile.
\item Insert the code fragment and run the test suite.
\end{enumerate}

While still somewhat similar to its ancestor GenProg, SearchRepair has a few changes that help it combat overfitting. 
Firstly, it copies and pastes larger sections of code. 
The intuition is that while a single line may behave very differently out of context, a multiline block of code is less likely to do so. 
SearchRepair would prefer to replace the entire buggy section with a correctly implemented version written by another human developer. 
Since this replacement block was not overfit to the test suite when it was written, there is less risk of it being overfit to the test suite in the repair. 
In addition, SearchRepair uses theorem proving to speed up this search. 
Although theorem proving is very slow, it is still much faster than running the test suite for every patch. 


SearchRepair generates patches with much higher quality than its purely random counterparts, passing 97.3\% of tests in an independent test suite, as opposed to GenProg's 68.7\%. 
Although SearchRepair did not fix as many bugs as GenProg, RSRepair, or AE, this may be because other methods fixed bugs simply by deleting functionality \cite{SearchRepair}, \cite{plausibility}. 


While its accuracy is admirable, SearchRepair has issues with scalability \cite{srthesis}. 
It was almost two orders of magnitude slower than the second slowest method run on the Introclass benchmark, and three orders of magnitude slower than all other methods. 
One of the main reasons for this is the way its theorem prover works. 
Because SearchRepair uses pieces of code from other applications as its patches, it must rename the variables to insert the code. 
However, it has no \textit{a priori} way of knowing what the correct variable mapping is, so it must run theorem proving on each possible mapping \cite{srthesis}. 

In the original work, this meant that the theorem prover was run $n!$ times on each patch, where n is the number of variables. 
Since SMT satisfiability is already an NP-Hard problem, SearchRepair effectively tried to brute-force an NP-Hard problem (variable mapping) that involves solving another NP-Hard problem (SMT satisfiability) on every iteration. 
This was, needless to say, very inefficient. 

Since the original work, the authors have found ways of speeding up this process. The mapping problem is now incorporated into the theorem proving, which allows all of the efficiencies of modern SAT solvers to be applied to mapping. However, the problem still remains that this method is not NP, since single-solution checking is NP-Hard (this assumes P =/= NP). 


While this process is prohibitively expensive for large numbers of patches, the time taken to test a single patch is not terribly high. 
Since most code fragments used by SearchRepair have no more than 4 or 5 variables, a single patch can often be tested in less than a second. 
It is only when very large databases of code fragments must be searched that problems arise.


\section{Methods}


As stated earlier, both SearchRepair and SPR/Prophet have drawbacks, either in their scalability or in their accuracy. 
For automatic program repair to become practically viable, a system must both create high-quality patches and be able to run in a reasonable amount of time.

We propose that one way to avoid the large computational cost associated with SearchRepair's semantic search while maintaining high accuracy is to use a fast heuristic to eliminate poor patch candidates. 
Using a heuristic to iterate over all the possible patches in a database lets us pick out those with a high likelihood of success, avoiding the majority of the computation previously required. 
This heuristic must be independent of variable mappings in order to keep its cost from becoming prohibitively expensive. 
The theorem prover would then run on these selected candidates, ensuring that each proposed patch is indeed of high quality.

To create such a heuristic, we begin with the feature extractor proposed by Long and Rinard in their Prophet paper \cite{Prophet}. 
This method creates a list of atoms (variables or constants) and operations the program uses on them. 
For instance, if a program checks \texttt{if(a > c \&\& a < b)}, then assigns \texttt{median = a}, the feature extractor would record \texttt{a: lessthan, greaterthan, < assign, R>}. 
The fact that \texttt{a} was on the right side of the assignment is recorded because assigning and being assigned to are very different operations. 

The complete list of features is enumerated below.
\begin{enumerate}
\item var: True if the atom is a variable
\item const0: True if the atom is a constant equal to 0
\item constn0: True if the atom is a constant not equal to 0

\item cond: True if the atom is in a conditional
\item iff: True if the atom is in the body or the conditional of an if statement
\item prt: True if the atom  is printed
\item loop: True if the atom is in the body or the conditional of a loop
\item EQ: True if the atom is in a statement with a ==
\item NEQ: True if the atom is in a statement with a !=

\item ret: True if the atom is returned

\item plus: True if the atom is in an addition statement
\item times: True if the atom is in a multiplication statement
\item minus\_l: True if the atom is on the left side of a subtraction operator
\item divided\_l: True if the atom is on the left side of a division operator
\item minus\_r: True if the atom is on the right side of a subtraction operator
\item divided\_r: True if the atom is on the right side of a division operator
\item increment: True if the atom is incremented
\item decrement: True if the atom is decremented
\item ASSIGN\_L: True if a value is assigned to the atom 
\item ASSIGN\_R: True if the atom is assigned to another variable

\item LESS\_THAN: True if the atom is in a statement with a \textless
\item GREATER\_THAN: True if the atom is in a statement with a \textgreater
\item LESS\_EQ: True if the atom is in a statement with a \textless =
\item GREATER\_EQ: True if the atom is in a statement with a \textgreater =
\end{enumerate}
 
These features are calculated for a given line (C), the three lines above it (P), and the three lines below it (N). 
The complete feature vector for a line in a program includes the set of features for C, P, and N for each atom in C. 

Since the goal is to evaluate whether a given patch will work for a given buggy section, we concatenate the feature vectors of the buggy code and the proposed patch to create a new vector, and feed this vector to the learner. 
However, this gives rise again to the issue of variable mappings. 
In the Prophet feature extractor, the features of an atom in the buggy program are explicitly encoded alongside the features of the corresponding variable in the patched program. 
Since encoding this mapping, even implicitly, would defeat the point of a mapping-free heuristic, we randomize the order in which atoms are encoded for every set of features, preventing the learner from finding mapping-dependent patterns. 
 
We train a random forest to classify patches as either correct or incorrect. 

Then, we use this classifier as a filter for what patches should be evaluated by the theorem prover. 
The modified algorithm is as follows.
\begin{enumerate}
\item Encode a database of human-written code fragments as satisfiability modulo theories (SMT)
\item Locate the bug
\item Build a functional description of each fragment that describes its input-output profile
\item Extract features of the buggy code
\item Extract features of each fragment in the database
\item Combine the buggy features and the fragment's features into a single patch feature vector
\item Apply the learner to each patch feature vector in the database. If the patch feature vector is classified as correct, add the corresponding code fragment to a second database
\item Search the new, filtered database for a code fragment that matches the desired input-output profile
\item Insert the code fragment and run the test suite 
\end{enumerate}


The following example serves to demonstrate the new algorithm. 

Suppose we have the following bug from the IntroClass benchmark: 

\begin{lstlisting}
/**/
#include <stdio.h>
#include <math.h>
int main(void)
{
	int int1, int2, int3, med; 
	printf("Please enter 3 numbers separated by spaces > ");
	scanf("%d %d %d", &int1, &int2, &int3);

	if (((int1 < int2) && (int1 > int3)) || ((int1 < int2) && (int1 > int3)))
	 	med = int1;
	else if ((((int2 < int1)) && (int2 > int3)) || ((int2 < int3) && (int2 > int1)))
        med = int2;
	else if (((int3 < int1) && (int3 > int2)) || ((int3 < int2) && (int3 > int1)))
	    med = int3;
    
    printf("%d is the median\n", med);
	return 0;
}                                             
\end{lstlisting}

Since this program uses$ > $and$ < $operators instead of$ >= $and$ <= $operators, it will fail in the case that two or more of the numbers are equal. 

Suppose then, that the modified SearchRepair's search space includes the following three patches

Patch 1
\begin{lstlisting}
while ((status = scanf("%c", &it)) != EOF && it != '\n') 
sum = (sum + (long) it) % 64;
sum = sum + (long) ' ';
printf("Check sum is %c\n", (char) sum);                                             
\end{lstlisting}

Patch 2
\begin{lstlisting}
for(i=flag1-1; i>=flag2; i--)
{ 
	if(flag2==1 && i==1)
	printf("-");
	printf("%c\n", digit[i]);
}
\end{lstlisting}

Patch 3
\begin{lstlisting}
if((a >= b && a <= c) || (a >= c && a <= b))
	median = a;
if((b >= a && b <= c) || (b >= c && b <= a))
	median = b;
else
	median = c;
printf("%d is the median\n", median);
\end{lstlisting}

Once the database has been constructed, the bug located, and the functional descriptions constructed, the algorithm begins the machine learning section.
 First, it constructs the feature vector for buggy section of code and for each of the patches. 
 Then, it concatenates the two vectors into a single vector which is fed to the learner. 
 In this case, let's presume that the learner classifies patch 2 and patch 3 as viable, but patch 1 as not viable.
  Patches 2 and 3 are added to the temporary database. 
  SearchRepair then fetches patch 2 from the database and runs the theorem prover on it.
   Since it doesn't match the functional specifications given by the test suite, it is discarded.
    SearchRepair fetches patch 3 from the database and applies the theorem prover to it as well.
     Since patch 3 fits the semantic constraints, it is accepted, and its variables are replaced with the variables from the buggy program.
      Finally it is tested against the test suite. 
      It passes, and the final patch is returned.

\begin{lstlisting}
/**/
#include <stdio.h>
#include <math.h>
int main(void)
{
	int int1, int2, int3; 
	printf("Please enter 3 numbers separated by spaces > ");
	scanf("%d %d %d", &int1, &int2, &int3);

	if((int1 >= int2 && int1 <= int3) || (int1 >= int3 && int1 <= int2))
		med = int1;
    if((int2 >= int1 && int2 <= int3) || (int2 >= int3 && int2 <= int1))
        med = int2;
    else
        med = int3;
    printf("%d is the median\n", med);
	return 0;
}                                             
\end{lstlisting}

\section{Evaluation}
Using the proposed feature vectors, we trained a random forest to classify pairs code snippets based on whether they were from programs that were trying to do the same thing. 
The intuition is that a program with similar functionality is far more likely to provide a correct patch than a program with very different functionality. 
These code snippets were sampled from the IntroClass benchmark. 
A pair was counted as a positive case if the two snippets were from programs submitted for the same assignment. 
Otherwise, they were counted as a negative case. 
The positive cases were sampled at a higher rate to make up for the asymmetry in the size of the classes. 

To evaluate how effective different machine learning methods were for this set of features, we took a subset of the data and tried an array of machine learning techniques to see which produced the best results.

\begin{table}[H]
\centering
\caption{Learner results}
\label{}
\begin{tabular}{lll}
\hline \hline
Algorithm & Training Time & Accuracy \\
\hline
SGD                 & 13.96 s 	& 51.297\% \\
Naive Bayes         & .21 s   	& 55.951\% \\
Bayes Net           & .76 s   	& 56.002\% \\
Random Forest       & 14.07 s 	& 86.165\% \\
J48                 & 27.22 s 	& 75.534\% \\
AdaBoost            & 6.58 s  	& 56.738\% \\
K-nearest neighbors & n/a    	& 81.587\% \\
SMO					& 540.51s	& 54.9593
\end{tabular}
\end{table}

Based on these results, we concluded that a random forest would be the most effective learner for this application. 
For this reason, the rest of our experiments are conducted using a random forest.

%\includegraphics[width=\textwidth]{one.JPG}

Using the full dataset, we train a random forest with 100 trees, run for 100 iterations, and test it using 10-fold cross-validation. 
The forest was correctly able to classify 96.17\% of the test cases. The false positive rate (classified as being from the same assignment, but really from different assignments) was 5.4\%. 
The false negative rate (classified as being from different assignments, but really from the same assignment)  was 2.4\%. 
For this application, this is a very satisfactory result. 

Next, the learner was incorporated into the SearchRepair pipeline, and the whole process was tested on the IntroClass benchmark. 
The learner described in the previous paragraph was used as the filter. 
At the time of writing, the full test without the filter has not yet finished, but has already run 6.83 times longer than it took the filtered version to complete. 
As such, we cannot provide a measure of accuracy as we do not yet know how many correct patches the filter ruled out. 
However, we do know that the filter was shown to have 97.6\% accuracy on the test dataset, and provides severalfold speedup over the unfiltered program. 
The authors consider this to be a worthwhile tradeoff.


\section{Future work}


There are several steps that could be taken to improve the filter. 
The next step would be to prepare it for a more general set of problems. 
To do this, the learner must be trained on real patches rather than distinguishing different purposes for code segments.
There are a few ways this could be done. 
The first would be to mine patches to existing bugs with SearchRepair, verify each by hand, and feed them to the learner to train on. 
This has the advantage that the filter would select for patches that SearchRepair is capable of making. 
Another method would be to mine existing bug fixes from GitHub. This does not require manual verification, but would select for patches that look like human patches, rather than for patches that look like correct SearchRepair patches. 


\section{CONCLUSIONS}

Automated program repair holds a great deal of promise, but it is rapidly becoming apparent that purely random methods are not sufficient for high-quality patches. 
Both patch synthesis methods like SPR and Prophet, and semantic constraint search-based methods such as SearchRepair have proven effective, but each has drawbacks as well. 
Combining Prophet's fast feature-based search with SearchRepair's slower, more reliable constraint-solving can produce a method that is both fast and accurate. 

\section*{Acknowledgement}
The authors would like to acknowledge the help of Afsoon Afzal for her implementation of SearchRepair and assistance with debugging.



\addtolength{\textheight}{-12cm}   % This command serves to balance the column lengths
                                  % on the last page of the document manually. It shortens
                                  % the textheight of the last page by a suitable amount.
                                  % This command does not take effect until the next page
                                  % so it should come on the page before the last. Make
                                  % sure that you do not shorten the textheight too much.



%\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv,saner-era2016}




\end{document}

